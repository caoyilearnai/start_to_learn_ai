# å¯¼å…¥æ‰€éœ€çš„åº“
import numpy as np
import faiss
import pickle
import os
# éœ€è¦å®‰è£… openai: pip install openai
from openai import OpenAI
from sentence_transformers import SentenceTransformer

# ----------------------------
# ä¸€ã€é…ç½®å‚æ•°
# ----------------------------

VECTOR_DB_DIR = "vector_db"
INDEX_PATH = os.path.join(VECTOR_DB_DIR, "faiss_index.bin")
DOCS_PATH = os.path.join(VECTOR_DB_DIR, "documents.pkl")
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
# é€šä¹‰åƒé—®æ¨¡å‹åç§° (è¯·æ ¹æ®ä½ åœ¨ç™¾ç‚¼å¹³å°é€‰æ‹©çš„æ¨¡å‹æ›´æ”¹)
GENERATION_MODEL_NAME = 'qwen-plus' # æˆ–è€… 'qwen-turbo', 'qwen-max', 'qwen-long'

# è®¾ç½® OpenAI é£æ ¼çš„å®¢æˆ·ç«¯ä»¥è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°
# è¯·å°† 'YOUR_DASHSCOPE_API_KEY' æ›¿æ¢ä¸ºä½ åœ¨é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°è·å–çš„å®é™… API Key
# æˆ–è€…è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY
api_key = os.getenv('DASHSCOPE_API_KEY')
if not api_key or api_key == 'YOUR_DASHSCOPE_API_KEY':
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æˆ–åœ¨ä»£ç ä¸­é…ç½®æœ‰æ•ˆçš„ API Key")

# é…ç½® OpenAI å®¢æˆ·ç«¯ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼æœåŠ¡
llm_client = OpenAI(
    api_key=api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# ----------------------------
# äºŒã€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (ä»…åµŒå…¥æ¨¡å‹)
# ----------------------------

print("ğŸ”§ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

# ----------------------------
# ä¸‰ã€å®šä¹‰æœ¬åœ°å‘é‡æ•°æ®åº“ç±»
# ----------------------------

class SimpleVectorDB:
    def __init__(self, dimension=384):
        self.dimension = dimension
        self.index = None
        self.documents = []
        self.is_loaded = False

    def create_and_save(self, documents):
        print("ğŸ§  æ­£åœ¨ä¸ºæ–‡æ¡£ç”Ÿæˆå‘é‡è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰...")
        doc_embeddings = embedding_model.encode(documents)
        doc_embeddings = np.array(doc_embeddings).astype('float32')

        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(doc_embeddings)

        os.makedirs(VECTOR_DB_DIR, exist_ok=True)
        faiss.write_index(self.index, INDEX_PATH)

        with open(DOCS_PATH, 'wb') as f:
            pickle.dump(documents, f)

        self.documents = documents
        self.is_loaded = True

        print(f"âœ… æˆåŠŸåˆ›å»ºå¹¶å‘é‡æ•°æ®åº“ä¿å­˜åˆ°:")
        print(f"   - ç´¢å¼•æ–‡ä»¶: {INDEX_PATH}")
        print(f"   - æ–‡æ¡£æ–‡ä»¶: {DOCS_PATH}")

    def load(self):
        if not os.path.exists(INDEX_PATH) or not os.path.exists(DOCS_PATH):
            raise FileNotFoundError(
                f"âŒ æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶ï¼\n"
                f"è¯·å…ˆè¿è¡Œ create_and_save() åˆ›å»ºæ•°æ®åº“ã€‚\n"
                f"éœ€è¦çš„æ–‡ä»¶:\n"
                f"  {INDEX_PATH}\n"
                f"  {DOCS_PATH}"
            )

        print("ğŸ“‚ æ­£åœ¨ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“...")
        self.index = faiss.read_index(INDEX_PATH)

        with open(DOCS_PATH, 'rb') as f:
            self.documents = pickle.load(f)

        self.is_loaded = True
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼")
        print(f"ğŸ“Š å…±åŠ è½½ {self.index.ntotal} ä¸ªæ–‡æ¡£å‘é‡")
        print(f"ğŸ“˜ å…± {len(self.documents)} æ¡åŸå§‹æ–‡æœ¬")

    def search(self, query, top_k=1):
        if not self.is_loaded:
            raise RuntimeError("âŒ æ•°æ®åº“è¿˜æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load() æˆ– create_and_save()")

        print(f"ğŸ” æ­£åœ¨æ£€ç´¢ä¸ '{query}' æœ€ç›¸å…³çš„æ–‡æ¡£...")
        query_embedding = embedding_model.encode([query])
        query_embedding = np.array(query_embedding).astype('float32')

        distances, indices = self.index.search(query_embedding, top_k)

        results = []
        for i in range(top_k):
            doc_idx = indices[0][i]
            if doc_idx == -1:
                continue
            text = self.documents[doc_idx]
            score = float(distances[0][i])
            results.append({'text': text, 'score': score})

        return results

# ----------------------------
# å››ã€RAG ä¸»å‡½æ•° (ä½¿ç”¨é€šä¹‰åƒé—® - OpenAI API é£æ ¼)
# ----------------------------

def rag_query(db, question, top_k=1, model_name=GENERATION_MODEL_NAME):
    """
    RAG æ ¸å¿ƒæµç¨‹ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ + è°ƒç”¨é€šä¹‰åƒé—®ç”Ÿæˆå›ç­” (OpenAI API é£æ ¼)
    :param db: å‘é‡æ•°æ®åº“å¯¹è±¡
    :param question: ç”¨æˆ·çš„é—®é¢˜
    :param top_k: æ£€ç´¢å‰ k ä¸ªç›¸å…³æ–‡æ¡£
    :param model_name: é€šä¹‰åƒé—®æ¨¡å‹åç§°
    :return: åŒ…å«é—®é¢˜ã€ä¸Šä¸‹æ–‡ã€å›ç­”çš„å­—å…¸
    """
    # 1. ä»æ•°æ®åº“ä¸­æ£€ç´¢ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æ¡£
    search_results = db.search(question, top_k=top_k)

    # 2. æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œæ‹¼æˆâ€œä¸Šä¸‹æ–‡â€
    retrieved_docs = [res['text'] for res in search_results]
    context = "\n".join(retrieved_docs) # ç”¨æ¢è¡Œç¬¦è¿æ¥

    # 3. æ„é€ æç¤ºè¯ï¼ˆPromptï¼‰ï¼šå‘Šè¯‰æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª helpful çš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ä»¥å›ç­”ï¼Œè¯·è¯´æ˜åŸå› ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚"},
        {"role": "user", "content": f"ä¿¡æ¯ï¼š\n{context}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"}
    ]

    # 4. è°ƒç”¨é€šä¹‰åƒé—®APIç”Ÿæˆå›ç­” (OpenAI API é£æ ¼)
    print("ğŸ¤– æ­£åœ¨è°ƒç”¨é€šä¹‰åƒé—®ç”Ÿæˆå›ç­”...")
    try:
        completion = llm_client.chat.completions.create(
            model=model_name,
            messages=messages,
            # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´å‚æ•°
            max_tokens=200,
            temperature=0.7,
            top_p=0.8
        )
        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        answer = completion.choices[0].message.content.strip()

    except Exception as e:
        print(f"âŒ è°ƒç”¨é€šä¹‰åƒé—®æ—¶å‡ºç°å¼‚å¸¸: {e}")
        answer = f"æŠ±æ­‰ï¼Œè°ƒç”¨æ¨¡å‹æ—¶å‡ºç°å¼‚å¸¸: {e}"

    # è¿”å›å®Œæ•´ç»“æœ
    return {
        "question": question,
        "retrieved_context": context,
        "messages": messages, # è¿”å›ç”¨äºè°ƒè¯•çš„ messages
        "answer": answer
    }


# ----------------------------
# äº”ã€ä¸»ç¨‹åºå…¥å£
# ----------------------------

if __name__ == "__main__":
    documents = [
        "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äº Web å¼€å‘ã€æ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½ã€‚",
        "RAG æ˜¯ Retrieval-Augmented Generation çš„ç¼©å†™ï¼Œç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆã€‚",
        "FAISS æ˜¯ Facebook å¼€æºçš„å‘é‡ç›¸ä¼¼æ€§æœç´¢åº“ï¼Œæ”¯æŒé«˜æ•ˆæ£€ç´¢ã€‚",
        "Sentence Transformers å¯ä»¥å°†å¥å­è½¬æ¢ä¸ºé«˜è´¨é‡çš„å‘é‡è¡¨ç¤ºã€‚",
        "é€šä¹‰åƒé—®æ˜¯ç”±é˜¿é‡Œäº‘å¼€å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€‚"
    ]

    db = SimpleVectorDB(dimension=384)

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    if not os.path.exists(INDEX_PATH) or not os.path.exists(DOCS_PATH):
        print("æœªæ‰¾åˆ°ç°æœ‰æ•°æ®åº“ï¼Œæ­£åœ¨åˆ›å»º...")
        db.create_and_save(documents)
    else:
        print("æ‰¾åˆ°ç°æœ‰æ•°æ®åº“ï¼Œæ­£åœ¨åŠ è½½...")
        db.load()

    # ç®€å•çš„äº¤äº’å¼é—®ç­”å¾ªç¯
    print("\n--- æ¬¢è¿ä½¿ç”¨åŸºäºé€šä¹‰åƒé—®(OpenAI API)çš„ç®€æ˜“RAGé—®ç­”ç³»ç»Ÿ ---")
    print("è¾“å…¥ 'é€€å‡º' æˆ– 'quit' ç»“æŸç¨‹åºã€‚")
    while True:
        user_input = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        if user_input.lower() in ['é€€å‡º', 'quit']:
            print("å†è§ï¼")
            break
        if user_input:
            try:
                result = rag_query(db, user_input, top_k=3)
                print(f"\nâ“ é—®é¢˜: {result['question']}")
                print(f"ğŸ“„ æ£€ç´¢åˆ°çš„ä¿¡æ¯: {result['retrieved_context']}")
                print(f"ğŸ’¡ é€šä¹‰åƒé—®å›ç­”: {result['answer']}")
            except Exception as e:
                print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        else:
            print("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆé—®é¢˜ã€‚")